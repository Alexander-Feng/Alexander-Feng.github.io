\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath, amssymb}
\title{Linear Algebra Notes}
\author{Alex Feng}
\date{June 2023}

\begin{document}

\maketitle

\section{Linear Equations}
\subsubsection*{Definition: Echelon Form}
A rectangular matrix is in echelon form (or row echelon form/REF) if it has the following properties:
\\\indent 1. All nonzero rows are above any rows of all zeros
\\\indent 2. Each leading entry of a row is in a column to the right of a leading entry \indent  of the row above it.
\\\indent 3. All entries in a column below a leading entry are zeros. 
\\A matrix is in reduced echelon form (or reduced row echelon form/RREF) if it also satisfies the additional conditions:
\\\indent 4. The leading entry in each nonzero row is 1.
\\\indent 5. Each leading 1 is the only nonzero entry in its column. 
\\
$$\begin{bmatrix}
0 & 1 & 3 & 0 & 0 & 7 \\
0 & 0 & 0 & 1 & 0 & 9 \\
0 & 0 & 0 & 0 & 1 & 2 \\
0 & 0 & 0 & 0 & 0 & 0 
\end{bmatrix}$$
\begin{center}This is an example of a matrix in RREF form
\end{center}
\subsubsection*{Theorem 1: Uniqueness of the RREF}
Each matrix is row equivalent to one and only one reduced echelon matrix. 
\subsubsection*{Definition: Pivot Position}
A pivot position in a matrix $A$ is a location in A that corresponds to a leading 1 in the reduced echelon form of $A$. A pivot column is a column of $A$ that contains a pivot position. 
\subsubsection*{Convention: Free variables}
Parametric descriptions of solutions should be written in terms of free variables (variables corresponding to non-pivot columns). Basic variables correspond to pivot columns. 
\subsubsection*{Theorem 2: Existence and Uniqueness Theorem}
A linear system is consistent iff the rightmost column of the augmented matrix is not a pivot column. A consistent system will have either a unique solution (no free variables) or infinitely many solutions (at least one free variable).
\subsubsection*{Definition: Span}
If $\vec v_1, \dots, \vec v_p$ are in $\mathbb{R}^n$, then the set of all linear combinations of $\vec v_1, \dots, \vec v_p$ is denoted by $\text{Span} \{\vec v_1, \dots, \vec v_p \}$ and is called the subset of $\mathbb{R}^n$ spanned (or generated) by $\vec v_1, \dots, \vec v_p$.
\subsubsection*{Definition: Matrix Multiplication}
If $A$ is an $m \times n$ matrix, with columns $\vec a_1, \dots, \vec a_n,$ and if $\vec x$ is in $\mathbb{R} ^n$, then the product of $A$ and $\vec x$, denoted by $A\vec x$, is the linear combination of the columns of $A$ using the corresponding entries in $\vec x$ as weights. 
$$A \vec x = \begin{bmatrix}
    \vec a_1 & \vec a_2 & \cdots & \vec a_n
\end{bmatrix}
\begin{bmatrix}
    x_1 \\
    \vdots \\
    x_n \\
\end{bmatrix}
= x_1 \vec a_1 + x_2 \vec a_2 + \cdots + x_n \vec a_n$$

\subsubsection*{Theorem 3: Matrix Equation Solutions}
If $A$ is an $m \times n$ matrix, with columns $\vec a_1, \dots, \vec a_n$, and if $\vec b$ is in $\mathbb{R}^n$, the matrix equation 
$$A \vec x = \vec b$$
has the same solution set as the vector equation 
$$x_1 \vec a_1 + x_2 \vec a_2 + \cdots + x_n \vec a_n = \vec b$$
which also has the same solutions set as the system of linear equations whose augmented matrix is
$$\begin{bmatrix}
    \vec a_1 & \vec a_2 & \cdots & \vec a_n & \vec b 
\end{bmatrix}$$
\subsubsection*{Theorem 4: Logically Equivalent Matrix Statements}
Let $A$ be an $m \times n$ coefficient matrix. Then the following statements are either all true or all false for a particular $A$. 
\\\indent a. For each $\vec b$ in $\mathbb{R}^m$, the equation $A \vec x = \vec b$ has a solution.
\\\indent b. Each $\vec b$ in $\mathbb{R}^m$ is a linear combination of the columns of $A$
\\\indent c. The columns of $A$ span $\mathbb{R}^m$
\\\indent d. $A$ has a pivot position in every row. 
\subsubsection*{Theorem 5: Matrix Product Properties}
If $A$ is an $m \times n$ matrix, $\vec u$ and $\vec v$ are vectors in $\mathbb{R}^n$, and $c$ is a scalar, then:
$$A(\vec u + \vec v) = A \vec u + A \vec v$$
$$A(c \vec u) = c(A \vec u)$$

\subsubsection*{Theorem 6: Homogeneous Equation Solutions}
For a consistent equation $A\vec x = \vec b$, let $\vec p$ be a solution. The solution set of $A \vec x = \vec b$ is the set of all vectors of the form $\vec w = \vec p + \vec v_h$, where $\vec v_h$ is any solution of the homogeneous equation $A\vec x = 0$

\subsubsection*{Definition: Linear Dependence} 
An indexed set of vectors $\{\vec v_1, \dots, \vec v_p \}$ in $\mathbb{R}^n$ is said to be linearly independent if the vector equation 
$$x_1 \vec v_1 + x_2 \vec v_2 + \cdots + x_p \vec v_p = 0$$
has only the trivial solution. The set $\{\vec v_1, \dots, \vec v_p \}$ is said to be linearly dependent if there exist weights $c_1, \dots, c_p$, not all zero, such that 
$$c_1 \vec v_1  + c_2 \vec v_2 + \cdots + c_p \vec v_p  = 0$$

\subsubsection*{Theorem 7: Characterization of Linearly Dependent Sets}
An indexed set $S = \{ \vec v_1 , \dots , \vec v_p \}$ of two or more vectors is linearly dependent iff at least one of the vectors in $S$ is a linear combination of the others. 

\subsubsection*{Theorem 8: Number of Entries and Vectors for Linear Dependence}
If a set contains more vectors than there are entries in each vector, then the set is linearly dependent. That is, any set ${\vec v_1, \dots , \vec v_p}$ in $\mathbb{R}^n$ is linearly dependent if $p > n$. 

\subsubsection*{Theorem 9: Zero Vector and Linear Dependence}
If a set $S = {\vec v_1, \dots, \vec v_p}$ in $\mathbb{R} ^n$ contains the zero vector, then the set is linearly dependent. 

\subsubsection*{Definition: Linear Transform}
A transformation (or mapping) T is linear if
\\(i) $T(\vec u+ \vec v) = T(\vec u) + T(\vec v)$ for all $\vec u, \vec v$ in the domain of $T$;
\\(ii) $T(c \vec u) = cT(\vec u)$ for all scalars $c$ and all $\vec u$ in the domain of $T$.

\subsubsection*{Theorem 10: Unique Matrix for Linear Transformation}
Let $T$ : $\mathbb{R}^n \xrightarrow{} \mathbb{R}^m$ be a linear transformation. then there exists a unique matrix $A$ such that
$$T(\vec x) = A \vec x \text{  for all } \vec x \text{ in }\mathbb{R}^n$$
In fact, $A$ is the $m \times n$ matrix whose $j$th column is the vector $T( \vec e_j)$, where $\vec e_j$ is the $j$th column of the identity matrix in $\mathbb{R}^n$:
$$A = \begin{bmatrix}
    T (\vec e_1) & \cdots & T(\vec e_n) 
\end{bmatrix}$$

\subsubsection*{Definition: Onto}
A mapping $T$ : $\mathbb{R}^n \xrightarrow{} \mathbb{R}^m$ is said to be onto $\mathbb{R}^m$ if each $\vec b$ in $\mathbb{R}^m$ is the image of at least one $\vec x$ in $\mathbb{R}^n$.

\subsubsection*{Definition: One-to-one}
A mapping $T$ : $\mathbb{R}^n \xrightarrow{} \mathbb{R}^m$ is said to be one-to-one $\mathbb{R}^m$ if each $\vec b$ in $\mathbb{R}^m$ is the image of at most one $\vec x$ in $\mathbb{R}^n$.

\subsubsection*{Theorem 11: Linear Transformation Trivial Solution}
Let $T$ : $\mathbb{R}^n \xrightarrow{} \mathbb{R}^m$ be a linear transformation. Then $T$ is one-to-one iff the equation $T(\vec x) = 0$ has only the trival solution. 

\subsubsection*{Theorem 12: Conditions for One-to-one Transformation}
Let $T$ : $\mathbb{R}^n \xrightarrow{} \mathbb{R}^m$ be a linear transformation, and let $A$ be the standard matrix for $T$. then:
\\a. $T$ maps $\mathbb{R}^n$ onto $\mathbb{R}^m$ iff the columns of $A$ span $\mathbb{R}^m$:
\\b. $T$ is one-to-one iff the columns of $A$ are linearly independent. 

\section{Matrix Algebra}
\subsubsection*{Theorem 1: Matrix Arithmetic}
Let $A,B,\text{ and }C$ be matrices of the same size, and let $r \text{ and } s$ be scalars. 

a. $A + B = B + A$

b. $(A+B) + C = A + (B+C)$

c. $A + 0 = A$

d. $r(A+B) = rA + rB$

e. $(r+s)A = rA + sA$

f. $r(sA) = (rs)A$
\subsubsection*{Definition: Matrix Composition}
If $A$ is an $m \times n$ matrix, and if $B$ is an $n \times p$ matrix with columns $\vec b_1 , \dots, \vec b_p$, then the product $AB$ is the $m \times p$ matrix whose columns are $A \vec b_1, \dots , A \vec b_p$. That is, 
$$AB = A \begin{bmatrix}
    \vec b_1 & \vec b_2 & \cdots & \vec b_p
\end{bmatrix}
= \begin{bmatrix}
    A \vec b_1 & A \vec b_2 & \cdots & A \vec b_p
\end{bmatrix}$$

\subsubsection*{Theorem 2: Properties of Matrix Multiplication}
Let $A$ be an $m \times n$ matrix, and let $B$ and $C$ have sizes for which the indicated sums and products are defined. 

a. $A(BC) = (AB)C$ (associative law of multiplication)

b. $A(B+C) = AB + AC$ (left distributive law)

c. $b+C)A = BA + CA$ (right distributive law)

d. $r(AB) = (rA)B = A(rB)$ for any scalar $r$

e. $I_m A = A = A I_n$ (identity for matrix multiplication)

\subsubsection*{Theorem 3: Transpose Properties}
Let $A$ and $B$ denote matrices whose sizes are appropriate for the following sums and products. 

a. $(A^T)^T = A$

b. $(A + B)^T = A^T + B^T$

c. For any scalar $r$, $(rA)^T = rA^T$ 

d. $(AB)^T = B^T A^T$

\subsubsection*{Theorem 4: Two by Two Invertible Matrix}
Let $A = \begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix}$. If $ad - bc \neq 0$, then $A$ is invertible and
$$A^{-1} = \frac{1}{ad - bc} \begin{bmatrix}
    d & -b \\
    -c & a
\end{bmatrix}$$
If $ad - bc = 0$, then $A$ is not invertible. 

\subsubsection*{Theorem 5: Invertible Matrix Solutions}
If $A$ is an invertible $n \times m$ matrix, then for each $\vec b$ in $\mathbb{R}^n$, the equation $A \vec x = \vec b$ has the unique solution $\vec x = A^{-1} \vec b$

\subsubsection*{Theorem 6: Properties of Invertible Matrix}

a. If $A$ is an invertible matrix, then $A ^{-1}$ is invertible and
$$(A^{-1})^{-1} = A$$
b. If $A \text{ and } B$ are $n \times n$ invertible matrices, then so is $AB$, and the inverse of $AB$ is the product of the inverses of $A$ and $B$ in the reverse order. That is, 
$$(AB)^{-1} = B^{-1}A^{-1}$$
c. If A is an invertible matrix, then so is $A^T$, and the inverse of $A^T$ is the transpose of $A^{-1}$. That is, 
$$(A^T)^{-1} = (A^{-1})^T$$

\subsubsection*{Theorem 7: Conditions for Matrix Invertibility}
An $n \times n$ matrix $A$ is invertible iff $A$ is row equivalent to $I_n$, and in this case, any sequence of elementary row operations that reduced $A$ to $I_n$ also transforms $I_n$ into $A^{-1}$. 

\subsubsection*{Theorem 8: The Invertible Matrix Theorem}
Let $A$ be a square $n \times n$ matrix. Then the following statements are equivalent. That is, for a given $A$, the statements are either all true or all false.

a. $A$ is an invertible matrix.

b. $A$ is row equivalent to the $n \times n$ identity matrix. 

c. $A$ has $n$ pivot positions.

d. The equation $A \vec x = 0$ only has the trivial solution. 

e. The columns of $A$ form a linearly independent set. 

f. The linear transformation $\vec x \mapsto A \vec x$ is one-to-one. 

g. The equation $A \vec x = \vec b$ has at least one solution for each $\vec b$ in $\mathbb{R}^n$.

h. the columns of $A$ span $\mathbb{R}^n$. 

i. The linear transformation $\vec x \mapsto A \vec x$ maps $\mathbb{R}^n$ onto $\mathbb{R}^n$. 

j. There is an $n \times n$ matrix $C$ such that $CA = I$. 

k. There is an $n \times n$ matrix $D$ such that $AD = I$. 

l. $A^T$ is an invertible matrix. 

\subsubsection*{Theorem 9: Invertible Transformation}
Let $T$: $\mathbb{R}^n \xrightarrow{} \mathbb{R}^n$ be a linear transformation and let $A$ be the standard matrix for $T$. Then $T$ is invertible iff $A$ is an invertible matrix. In that case, the linear transformation $S$ given by $S( \vec x) = A^{-1} \vec x$ is the unique function satisfying $S(T(\vec x)) = \vec x$ for all $\vec x \text{ in }\mathbb{R}^n$ and $T(S(\vec x)) = \vec x$ for all $\vec x$ in $\mathbb{R}^n$

\subsubsection*{Theorem 10: Column-Row Expansion of $AB$}
If $A$ is $m \times n$ and $B$ is $n \times p$, then 
$$AB = \begin{bmatrix}
    \text{col}_1 (A) & \text{col}_2(A) & \cdots & \text{col}_n (A)
\end{bmatrix}
\begin{bmatrix}
    \text{row}_1 (B) \\
    \text{row}_2 (B) \\
    \vdots \\
    \text{row}_n (B)
\end{bmatrix}$$
$$= \text{col}_1(A)\text{row}_1 (B) + \cdots + \text{col}_n (A) \text{row}_n (B)$$

\section{Determinants}
\subsubsection*{Definition: Determinant}
For $n\geq 2$, the determinant of an $n \times n$ matrix $A = \begin{bmatrix}
    a_{ij}
\end{bmatrix}$ is the sum of $n$ terms of the form $\pm a_{1j} \det A_{1j}$, with plus and minus signs alternating, where the entries $a_{11}, a_{12}, \dots , a_{1n}$ are from the first row of $A$. In symbols, 
$$\det A = a_{11} \det A_{11} - a_{12} \det A_{12} + \cdots + (-1)^{1+n} a_{1n} \det A_{1n}$$
$$= \sum_{j=1}^{n} (-1)^{1+j} a_{1j} \det A_{1j}$$

\subsubsection*{Theorem 1: Cofactor}
The determinant of an $n \times n$ matrix $A$ can be computed by a cofactor expansion across any row or down any column. The expansion across the $i$th row using the cofactors in $C_{ij} = (-1)^{i+j} \det A_{ij}$ is
$$\det A = a_{i1} C_{i1} + a_{i2} C_{i2} + \cdots + a_{in} C_{in}$$
The cofactor expansion down the $j$th column is 
$$\det A = a_{1j} C_{1j} + a_{2j} C_{2j} + \cdots + a_{nj} C_{nj}$$

\subsubsection*{Theorem 2: Determinant of Triangular Matrix}
If $A$ is a triangular matrix, then $\det A$ is the product of the entries on the main diagonal of $A$. 

\subsubsection*{Theorem 3: Row Operations for Determinants}
Let $A$ be a square matrix

a. If a multiple of one row of $A$ is added to another row to produce a matrix \indent $B$, then $\det B = \det A$. 

b. If two rows of $A$ are interchanged to produce $B$, then $\det B = -\det A$

c. If one row of $A$ is multiplied by $k$ to produce $B$, then $\det B = k \det A$

\subsubsection*{Theorem 4: Determinants and Invertibility}
A square matrix $A$ is invertible iff $\det A \neq 0$

\subsubsection*{Theorem 5: Determinant of a Transpose}
If $A$ is an $n \times n$ matrix, then $\det A^T = \det A$

\subsubsection*{Theorem 6: Multiplicative Property}
If $A$ and $B$ are $n \times n$ matrices, then $\det AB = (\det A)(\det B)$

% \subsubsection*{Theorem 7: Cramer's Rule}
% Let $A$ be an invertible $n \times n$ matrix. For any $\vec b$ in $\mathbb{R}^n$, the unique solution $\vec x$ of $A \vec x = \vec b$ has entries given by
% $$x_i = \frac{\det A_i (\vec b)}{\det A}, \hspace{10pt} i = 1,2,\dots, n$$

\section{Vector Spaces and Subspaces}
\subsubsection*{Definition: Vector Space}
A vector space is a nonempty set $V$ of objects, called vectors, on which are defined two operations, called addition and multiplication by scalars (real numbers), subject to the ten axioms listed below. The axioms must hold for all vectors $\vec u$, $\vec v$, and $\vec w$ in $V$ and for all scalars $c$ and $d$. 

1. The sum of $\vec u$ and $\vec v$, denoted by $\vec u + \vec v$, is in $V$. 

2. $\vec u + \vec v = \vec v + \vec u$.

3. $(\vec u + \vec v) + w = \vec u + (\vec v + \vec w)$.

4. There is a zero vector $0$ in $V$ such that $\vec u + 0 = \vec u$.

5. For each $u$ in $V$, there is a vector $-u$ in $V$ such that $\vec u + (-\vec u) = 0$.

6. $c \vec u$ is in V

7. $c(\vec u + \vec v) = c \vec u + d \vec u$

8. $(c + d) \vec u = c \vec u + d \vec u$

9. $c(d \vec u) = (cd) \vec u$

10. $1\vec u = \vec u$

\subsubsection*{Definition: Subspace}
A subspace of a vector space $V$ is a subset $H$ of $V$ that has three properties: 

a. The zero vector of $V$ is in $H$.

b. $H$ is closed under vector addition. That is, for each $\vec u$ and $\vec v$ in $H$, the \indent sum $\vec u + \vec v$ is in $H$.

c. $H$ is closed under multiplication by scalars. That is, for each $\vec u$ in $H$ and \indent each scalar $c$, the vector $c \vec u$ is in $H$.

\subsubsection*{Theorem 1: Subspace and Span}
If $\vec v_1 , \dots , \vec v$ are in vector space $V$, then Span$\{ \vec v_1 , \dots , \vec v_P \}$ is a subspace of $V$

\subsubsection*{Definition: Null Space}
The null space of an $m \times n$ matrix $A$, written as Nul$A$ or $\mathcal{N}(A)$, is the set of all solutions of the homogeneous equation $A \vec x = 0$. In set notation,
$$\mathcal{N} (A) = \{\vec x: \vec x \text{ is in }\mathbb{R}^n \text{ and } A \vec x = 0 \}$$

\subsubsection*{Theorem 2: Null Space Subspace}
The null space of an $m \times n$ matrix $A$ is a subspace of $\mathbb{R}^n$. Equivalently, the set of all solutions to a system $A\vec x = 0$ of $m$ homogeneous linear equations in $n$ unknowns is a subspace of $\mathbb{R}^n$.

\subsubsection*{Definition: Column Space}
The column space of an $m \times n$ matrix $A$, written as $\text{Col}A$ or $\mathcal{C}(A)$, is the set of all linear combinations of the columns of $A$. If $A= \begin{bmatrix}
    \vec a_1 & \cdots & \vec a_n
\end{bmatrix}$, then 
$$\mathcal{C}(A) = \text{Span}\{ \vec a_1, \dots , \vec a_n \}$$

\subsubsection*{Theorem 3: Column Space Subspace}
The column space of an $m \times n$ matrix $A$ is a subspace of $\mathbb{R}^m$

\subsubsection*{Definition: Linear Transformation}
A linear transformation $T$ from a vector space $V$ into a vector space $W$ is a rule that assigns to each vector $\vec x$ in $V$ a unique vector $T(\vec x)$ in $W$, such that

(i)\hspace{6pt}$T(\vec u + \vec v) = T(\vec u) + T(\vec v)$ \hspace{12 pt} for all $\vec u$, $\vec v$ in $V$, and

(ii) $T(c\vec u) = cT(\vec u)$ \hspace{53 pt} for all $\vec u$ in $V$ and all scalars $c$. 

\subsubsection*{Theorem 4: Linear Dependence}
An indexed set $\{ \vec v_1 , \dots , \vec v_p \}$ of two or more vectors, with $\vec v_1 \neq 0$, is linearly dependent iff some $\vec v_j$ (with $j > 1$) is a linear combination of the preceding vectors, $\vec v_1, \dots , \vec v_{j-1}$.

\subsubsection*{Definition: Basis}
Let $H$ be a subspace of vector space $V$. A set of vectors $\beta$ in $V$ is a basis for $H$ if

(i)\hspace{6pt}$\beta$ is a linearly independent set, and

(ii) the subspace spanned by $\beta$ coincides with $H$; that is,
$$H = \textrm{Span} \beta$$

\subsubsection*{Theorem 5: The Spanning Set Theorem}
Let $S = \{ \vec v_1 , \dots \vec v_p \}$ be a set in a vector space $V$, and let $H = \text{Span}\{ \vec v_1 ,\dots, \vec v_p \}$. 

a. If a vector $\vec v_k \in S$ is a linear combination of the remaining vectors in $S$, $$\text{Span}(S - \{\vec v_k\}) = \text{Span}H$$

b. If $H \neq \{ 0 \}$, some subset of $S$ is a basis for $H$.

\subsubsection*{Theorem 6: Basis of Column Space}
The pivot columns of a matrix $A$ form a basis for $\mathcal{C}(A)$.

\subsubsection*{Theorem 7: Basis of Row Space}
If two matrices $A$ and $B$ are row equivalent, then their row spaces are the same. If $B$ is in echelon form, the nonzero rows of $B$ form a basis for the row space of $A$ as well as for that of $B$. 

\subsubsection*{Theorem 8: The Unique Representation Theorem}
Let $\beta = \{ \vec b_1 , \dots, \vec b_n \}$ be a basis for a vector space $V$. Then for each $\vec x$ in $V$, there exists a unique set of scalars $c_1, \dots , c_n$ such that
$$\vec x = c_1 \vec b_1 + \cdots + c_n \vec b_n$$

\subsubsection*{Definition: Basis Coordinates}
Supposed $\beta = \{ \vec b_1 , \dots ,\vec b_n \}$ is a basis for a vector space $V$ and $\vec x \in V$. The coordinates of $\vec x$ relative to the basis (or the $\beta$-coordinates of $\vec x$) are the weights of $c_1 , \dots , c_n$ such that $\vec x = c_1 \vec b_1 + \cdots + c_n \vec b_n$

\subsubsection*{Theorem 9: Coordinate Mapping One-to-One}
Let $\beta = \{\vec b_1 , \dots , \vec b_n \}$ be a basis for a vector space $V$. Then the coordinate mapping $\vec x \mapsto [ \vec x ] _\beta$ is a one-to-one linear transformation from $V$ onto $\mathbb{R}^n$.

\subsubsection*{Theorem 10: Basis and Linear Dependence}
If a vector space $V$ has a basis $\beta = \{ \vec b_1 , \dots , \vec b_n \}$, then any set in $V$ containing more than $n$ vectors must be linearly dependent. 

\subsubsection*{Theorem 11: Entries in Basis}
If a vector space $V$ has a basis of $n$ vectors, then every basis of $V$ must consist of exactly $n$ vectors. 

\subsubsection*{Definition: Dimension}
Let $V$ be a vector space. $V$ is finite-dimensional if it is spanned by a finite set. The dimension of $V$ (written as $\dim V$) is the number of vectors in a basis for $V$. $\dim (\{0\}) \equiv 0$. $V$ is infinite-dimensional if it is not spanned by a finite set. 

\subsubsection*{Theorem 12: Counterpart to Spanning Set Theorem}
Let $H$ be a subspace of a finite-dimensional vector space $V$. Any linearly independent set in $H$ can be expanded, if necessary, to a basis for $H$. Also, $H$ is finite-dimensional and 
$$\dim H \leq \dim V$$

\subsubsection*{Definition: Rank and Nullity}
The rank of an $m \times n$ matrix $A$ is $\dim \mathcal{C}(A)$ and the nullity of $A$ is $\dim \mathcal{N}(A)$.

\subsubsection*{Theorem 14: The Rank Theorem}
Let $A$ be an $m \times n$ matrix.
$$\text{rank }A + \text{nullity }A = \text{number of columns in }A$$

\subsubsection*{Theorem: The Invertible Matrix Theorem (cont.)}
Let $A$ be an $n \times n$ matrix. The following statements are equivalent the the statement that $A$ is an invertible matrix. 

m. The columns of $A$ form a basis of $\mathbb{R}^n$

n. $\mathcal{C}(A) = \mathbb{R}^n$

o. rank $A=n$

p. nullity $A = 0$

q. $\mathcal{N}(A) = \{ 0 \}$

\subsubsection*{Theorem 15: Change of Coordinates Matrix}
Let $\beta = \{ \vec b_1 , \dots , \vec b_n \}$ and $\gamma = \{ \vec g_1 , \dots , \vec g_n \}$ be bases of a vector space $V$. Then there is a unique $n \times n$ matrix $\mathop{P}_{\gamma \leftarrow \beta}$ such that
$$\left[ \vec x \right]_\gamma = \mathop{P}\limits_{\gamma \leftarrow \beta} \left[ \vec x \right]_\beta$$
The columns of $\mathop{P}_{\gamma \leftarrow \beta}$ are the $\gamma$-coordinate vectors of the vectors in the basis $\beta$. That is
$$\mathop{P}\limits_{\gamma \leftarrow \beta} = \begin{bmatrix}
    [\vec b_1]_\gamma & [\vec b_2]_\gamma & \cdots & [\vec b_n]_\gamma
\end{bmatrix}$$

\section{Eigenvalues and Eigenvectors}
\subsubsection*{Definition: Eigenvalues and Eigenvectors}
An eigenvector of an $n \times n$ matrix $A$ is a nonzero vector $\vec x$ such that $A \vec x = \lambda \vec x$ for some scalar $\lambda$. A scalar $\lambda$ is called an eigenvalue of $A$ if there is a nontrivial solution $\vec x$ of $A \vec x = \lambda \vec x$; such an $\vec x$ is called an eigenvector corresponding to $\lambda$.

\subsubsection*{Theorem 1: Eigenvalues of Triangular Matrix}
The eigenvalues of a triangular matrix are the entries on its main diagonal.

\subsubsection*{Theorem 2: Eigenvalues and Linear Independence}
If $\vec v_1 , \dots , \vec v_r$ are eigenvectors that correspond to distinct eigenvalues $\lambda_1 , \dots , \lambda _r$ of an $n \times n$ matrix $A$, then the set $\{ \vec v_1 , \dots , \vec v_r \}$ is linearly independent. 

\subsubsection*{Theorem 3: Properties of Determinants}
$A, B \in M_{n \times n}$

a. $A$ is invertible iff $\det A \neq 0$

b. $\det AB = \det(A)\det(B)$

c. $\det A^T = \det A$

d. If $A$ is triangular, $\det A = \prod_{k = 1}^{n} a_{kk}$

e. A row replacement operation on $A$ does not change $\det A$. A row interchange changes the sign of the determinant. A row scaling also scales the determinant by the same scalar factor. 

\subsubsection*{Theorem: The Invertible Matrix Theorem (cont.)}
$A \in M_{n \times n}$. $A$ is invertible iff

r. $0$ is not en eigenvalue of $A$.

\subsubsection*{Theorem 4: Eigenvalues and Similarity}
$A, B \in M_{n \times n}$. If $A$ and $B$ are similar then they have the same characteristic polynomial and hence the same eigenvalues (with the same multiplicities).

\subsubsection*{Theorem 5: The Diagonalization Theorem}
$A \in M_{n \times n}$. $A$ is diagonalizable iff it has $n$ linearly independent eigenvectors. 
In fact, $A = P D P^{-1}$, with $D$ a diagonal matrix, iff the columns of $P$ are $n$ linearly independent eigenvectors of $A$. In this case, the diagonal entries of $D$ are eigenvalues of $A$ that correspond, respectively, to the eigenvectors in $P$.

\subsubsection*{Theorem 6: Eigenvalues and Diagonalizability}
If $A \in M_{n \times n}$ has $n$ distinct eigenvalues then it is diagonalizable. 

\subsubsection*{Theorem 7: Eigenvectors and Dimension}
Let $A \in M_{n \times n}$ have distinct eigenvalues $\lambda_1 , \dots , \lambda_p$ \\
a. For $1 \leq k \leq p$, the dimension of the eigenspace for $\lambda_k$ is less than or equal to the mulitiplicity of eigenvalue $\lambda _k$ \\
b. $A$ is diagonazliable iff the sum of the dimensions of the eigenspaces equals $n$. \\
c. If $A$ is diagonalizable and $\beta_k$ is a basis for the eigenspace corresponding to $\lambda_k$ for each $k$, then the total collection of vectors in the sets $\beta_1 , \dots , \beta_p$ forms an eigenvector basis for $\mathbb{R}^n$.

\subsubsection*{Definition: Eigenvector Transformation}
Let $V$ be a vector space. An eigenvector of a linear transformation $T: V \rightarrow V$ is $\vec x \in V, \vec x \neq 0$ such that $T(\vec x) = \lambda \vec x$ for some scalar $\lambda$. A scalar $\lambda$ is called an eigenvalue of $T$ if there is a nontrivial solution $\vec x$ of $T(\vec x) = \lambda \vec x$; such an $\vec x$ is called an eigenvector corresponding to $\lambda$.

\subsubsection*{Theorem 8: Diagonal Matrix Representation}
Suppose $A = P D P^{-1}$, where $D \in M_{n \times n}$ is diagonal. If $\beta$ is the basis for $\mathbb R ^n$ formed from the columns of $P$, then $D$ is the $\beta$-matrix for the transformation $\vec x \mapsto A \vec x$.

\section{Orthogonality}
\subsubsection*{Theorem 1: Inner Product Properties}
Let $\vec u, \vec v, \text{ and } \vec w \in \mathbb R^n , c \in \mathbb R$. 

a. $\vec u \cdot \vec v = \vec v \cdot \vec u$

b. $(\vec u + \vec v) \cdot \vec w = \vec u \cdot \vec w + \vec v \cdot \vec w$

c. $(c \vec u) \cdot \vec v = c( \vec u \cdot \vec v ) = \vec u \cdot (c \vec v)$

d. $\vec u \cdot \vec u \geq 0$, and $\vec u \cdot \vec u =0 \iff \vec u = 0$ 

\subsubsection*{Definition: Norm}
The norm (length) of $\vec v$ is $\| \vec v\| \in \mathbb R _{\geq 0}$ defined by 
$$\|\vec v\| = \sqrt{\vec v \cdot \vec v}$$

\subsubsection*{Definition: Distance}
For $\vec u , \vec v \in \mathbb R^n$, the distance between $\vec u$ and $\vec v$, written as $\text{dist}(\vec u, \vec v)$, is the norm of vector $\vec u - \vec v$, or 
$$\text{dist}(\vec u, \vec v) = \| \vec u - \vec v\|$$

\subsubsection*{Definition: Orthogonal}
$\vec u, \vec v \in \mathbb R^n$ are orthogonal to each other if $\vec u \cdot \vec u = 0$.

\subsubsection*{Theorem 2: The Pythagorean Theorem}
$\vec v , \vec u \in \mathbb R^n$ are orthogonal to each other iff $\| \vec u + \vec u\|^2 = \|\vec u\|^2 + \|\vec v\|^2$.

\subsubsection*{Theorem 3: Null Space Orthogonal Complement}
Let $A \in M_{m \times n}$ 
$$(\text{Row}(A))^\perp = \mathcal N (A) \text{  and  } (\mathcal C (A) )^\perp = \mathcal N (A^T)$$

\subsubsection*{Theorem 4: Orthogonal Set Subspace}
If $S = \{ \vec u_1 , \dots, \vec u_p \}$ is an orthogonal set of nonzero vectors in $\mathbb R^n$, then $S$ is linearly independent and hence is a basis for the subspace spanned by $S$. 

\subsubsection*{Definition: Orthogonal Basis}
An orthogonal basis for a subspace $W$ of $\mathbb R^n$ is a basis for $W$ that is also an orthogonal set. 

\subsubsection*{Theorem 5: Linear Combination of Orthogonal Basis}
Let $\{ \vec u_1 , \dots , \vec u_p \}$ be an orthogonal basis for a subspace $W$ of $\mathbb R^n$. For each $\vec y \in W$, 
$$\vec y = c_1 \vec u_1 + \cdots + c_p \vec u_p$$
$$c_j = \frac{\vec y \cdot \vec u_j}{\vec u_j \cdot \vec u_j} \hspace{10 pt} (j = 1, \dots , p)$$

\subsubsection*{Theorem 6: Orthonormal Columns}
$U \in M_{m \times n}$ has orthonormal columns iff $U^T U = I$

\subsubsection*{Theorem 7: Orthonormal Column Properties}
$U \in M_{m \times n}$ with orthonormal columns, $\vec x , \vec y \in \mathbb R^n$. 

a. $\|U \vec x\| = \| \vec x\|$

b. $(U \vec x)\cdot (U \vec y ) = \vec x \cdot \vec y$

c. $(U \vec x)\cdot (U \vec y) = 0 \iff \vec x \cdot \vec y = 0$

\subsubsection*{Theorem 8: The Orthogonal Decomposition Theorem}
Let $W$ be a subspace of $\mathbb R ^n$. Each $\vec y \in \mathbb R^n$ can be written uniquely in the form 
$$\vec y = \hat y + \vec z$$
where $\hat y \in W, \vec z \in W^\perp$. In fact, if $\{\vec u_1 , \dots , \vec u_p \}$ is any orthogonal basis of $W$, then 
$$\hat y = \frac{\vec y \cdot \vec u_1}{\vec u_1 \cdot \vec u_1} \vec u_1 + \cdots + \frac{\vec y \cdot \vec u_p}{\vec u_p \cdot \vec u_p} \vec u_p$$
and $\vec z = \vec y -\hat y$.

\subsubsection*{Theorem 9: The Best Approximation Theorem}
Let $W$ be a subspace of $\mathbb R^n$, $\vec y \in \mathbb R^n$, $\hat y = \text{proj}_W \vec y$
$$\forall \vec v \in W, \vec v \neq \hat y \implies \|\vec y - \hat y \| < \|\vec y - \vec v\|$$

\subsubsection*{Theorem 10: Projection onto a Subspace}
If $\{ \vec u _1 , \dots , \vec u_p \}$ is an orthonormal basis for the subspace $W$ of $\mathbb R ^n$, then 
$$\text{proj}_W \vec y = (\vec y \cdot \vec u_1)\vec u_1 +(\vec y \cdot \vec u_2)\vec u_2 + \cdots + (\vec y \cdot \vec u_p)\vec u_p$$
$$U = \begin{bmatrix}
    \vec u_1 & \vec u_2 &\cdots & \vec u_p
\end{bmatrix} \implies \forall \vec y \in \mathbb R^n ,\text{proj}_W \vec y = UU^T \vec y$$

\subsubsection*{Theorem 11: The Gram-Schmidt Process}
Given a basis $\{ \vec x_1 , \dots , \vec x_p \}$ for a nonzero subspace $W$ of $\mathbb R^n$, define

$$\vec v_1 = \vec x_1$$

$$\vec v_2 = \vec x_2 - \frac{\vec x_2 \cdot \vec v_1}{\vec v_1 \cdot \vec v_1} \vec v_1$$

$$\vec v_3 = \vec x_3 - \frac{\vec x_3 \cdot \vec v_1}{\vec v_1 \cdot \vec v_1} \vec v_1 -\frac{\vec x_3 \cdot \vec v_2}{\vec v_2 \cdot \vec v_2} \vec v_2$$

$$\vdots$$

$$\vec v_p = \vec x_p - \frac{\vec x_p \cdot \vec v_1}{\vec v_1 \cdot \vec v_1} \vec v_1 -\frac{\vec x_p \cdot \vec v_2}{\vec v_2 \cdot \vec v_2} \vec v_2 - \cdots - \frac{\vec x_p \cdot \vec x_{p-1}}{\vec v_{p-1} \cdot \vec v_{p-1}} \vec v_{p-1}$$
Then $\{ \vec v_1 , \dots , \vec v_p$ is an orthogonal basis for $W$. In addition
$$\text{Span}\{\vec v_1 , \dots , \vec v_k \} =\text{Span}\{\vec x_1 , \dots , \vec x_k \} \hspace{10pt} \text{ for } 1 \geq k \geq p$$

\subsubsection*{Theorem 12: The QR Factorization}
If $A \in M _{m \times n}$ with linearly independent columns, then $A$ can be factored as $A = QR$, where $Q \in M_{m \times n}$ matrix whose columns form an orthonormal basis for $\mathcal C (A)$ and $R \in M_{n \times n}$ is an upper triangluar invertible matrix with positive entries on its diagonal. 

\subsubsection*{Definition: Least-Squares Solution}
If $A \in M_{m \times n}, \vec b \in \mathbb R^m$, a least-squares solution of $A \vec x = \vec b$ is an $\hat x \in \mathbb R^n$ such that
$$\forall \vec x \in \mathbb R^n, \|\vec b - A \hat \vec x \| \leq \| \vec b - A \vec x \|$$

\subsubsection*{Theorem 13: Least-Squares Solution Set}
The set of least-squares solutions of $A \vec x = \vec b$ coincides with the nonempty set of solutions of the normal equations $A ^T A \vec x = A^T \vec b$.

\subsubsection*{Theorem 14: Least-Squares Statments}
$A \in M_{m \times n}$. The following statements are logically equivalent:

a. The equation $A \vec x = \vec b$ has a unique least-squares solution for each $\vec b \in \mathbb R^m$.

b. The columns of $A$ are linearly independent.

c. The matrix $A^T A$ is invertible \\
When these statements are true, the least-squares solution $\hat x$ is given by 
$$\hat x = (A^T A)^{-1} A^T \vec b$$

\subsubsection*{Theorem 15: QR Factorization and Least-Squares Solution}
Given $A \in M_{m \times n}$ with linearly independent columns, let $A = QR$ be a $QR$ factorization of $A$ as in Theorem 12. Then, for each $\vec b \in \mathbb R^m$, the equation $A \vec x = \vec b$ has a unique least-squares solution, given by
$$\hat x = R^{-1} Q^T \vec b$$

\subsubsection*{Definition: Inner Product Space}
An inner product on a vector space $V$ is a function that, to each pair of vectors $\vec u, \vec v \in V$, associates a real number $\langle \vec u, \vec v \rangle$ and satisfies the following axioms, $\forall \vec u, \vec v, \vec w \in V, \forall c \in \mathbb R$:

1. $\langle \vec u, \vec v \rangle = \langle \vec v, \vec u \rangle$

2. $\langle \vec u + \vec v , \vec w \rangle = \langle \vec u, \vec w \rangle + \langle \vec v, \vec w \rangle$

3. $\langle c \vec u , \vec v \rangle = c\langle \vec u , \vec v \rangle$

4. $\langle \vec u , \vec u \rangle \geq 0 \text{ and } \langle \vec u, \vec u \rangle = 0 \iff \vec u = 0$
\\A vector space with an inner product is called an inner product space. 

\subsubsection*{Theorem 16: The Cauchy-Schwarz Inequality}
$$\forall \vec u, \vec v \in V, |\langle \vec u, \vec v \rangle | \leq \|\vec u \| \|\vec v \|$$

\subsubsection*{Theorem 17: The Triangle Inequality}
$$\forall \vec u, \vec v \in V, \| \vec u + \vec v \| \leq \| \vec u \| + \| \vec v \|$$

\section{Symmetric Matrices and Quadratic Forms}
\subsubsection*{Theorem 1: Orthogonal Eigenvectors}
If $A$ is symmetric, then any two eigenvectors form different eigenspaces are orthogonal. 

\subsubsection*{Theorem 2: Orthogonal Diagonalizability}
$A \in M_{n \times n}$ is orthogonally diagonalizable iff $A$ is a symmetric matrix. 

\subsubsection*{Theorem 3: The Spectral Theorem for Symmetric Matrices}
If $A \in M_{n \times n}$ is symmetric:

a. $A$ has $n$ real eigenvalues, counting multiplicities

b. The dimension of the eigenspace for each eigenvalue $\lambda$ equals the multiplicity of $\lambda$ as a root of the characteristic equation.

c. The eigenspaces are mutually orthogonal

d. $A$ is orthogonally diagonalizable. 

\subsubsection*{Theorem 4: The Principal Axes Theorem}
Let $A \in M_{n \times n}$ be symmetric. Then there is an orthogonal change of variable, $\vec x = P \vec y$, that transforms the quadratic form $\vec x^T A \vec x$ into a quadratic form $\vec y^T D \vec y$ with no cross-product term. 
\end{document}


